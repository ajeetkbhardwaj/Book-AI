For a matrix input (X), the dense layer applies the same formula but using matrix multiplication:

[
Z = \sigma(XW + b)
]

Shapes:

* (X): ((N, d)) — batch size (N), input features (d)
* (W): ((d, h)) — (h) hidden units
* (b): ((h,)) — one bias per hidden unit
* Output (Z): ((N, h))

Example: batch of 4 samples, 3 features → 2-neuron layer.

```python
import numpy as np

def sigmoid(x):
    return 1/(1+np.exp(-x))

def linear(x, w, b):
    return x @ w + b

def ff_layer(x, w, b):
    return sigmoid(linear(x, w, b))

if __name__ == "__main__":
    # input matrix (4 samples, 3 features)
    X = np.array([
        [0, 1, 2],
        [1, 0, 1],
        [2, 3, 4],
        [4, 5, 6]
    ], dtype=float)

    # weight matrix (3 input features -> 2 neurons)
    W = np.array([
        [0.2, 0.1],
        [0.3, 0.4],
        [0.5, 0.6]
    ], dtype=float)

    # bias vector for 2 neurons
    b = np.array([0.1, -0.2], dtype=float)

    Z = ff_layer(X, W, b)
    print(Z)
```

Explanation:

* `X @ W` computes weighted sums for each sample.
* `b` broadcasts across rows.
* `sigmoid` activates each neuron.

References:

* *Deep Learning*, Goodfellow et al., Section 6.4 (Matrix form of forward pass)
* Stanford CS231n, Fully Connected Layers: [https://cs231n.github.io/neural-networks-case-study/#layers](https://cs231n.github.io/neural-networks-case-study/#layers)
* NumPy Broadcasting: [https://numpy.org/doc/stable/user/basics.broadcasting.html](https://numpy.org/doc/stable/user/basics.broadcasting.html)

Ask if you want the ReLU or softmax version or backprop derivation.
